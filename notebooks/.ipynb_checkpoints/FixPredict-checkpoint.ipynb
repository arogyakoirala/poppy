{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76e90cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arogya/.local/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "*** Earth Engine *** FINAL DEADLINE: ee.Authenticate will fail after 2022-06-06. Please upgrade. https://developers.google.com/earth-engine/guides/python_install\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://swanlund.space/parallelizing-python\n",
    "from datetime import date, timedelta\n",
    "import eeconvert as eeconvert\n",
    "import ee\n",
    "import geemap\n",
    "import multiprocessing as mp\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "from progress.bar import Bar\n",
    "from pathlib import Path\n",
    "\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.mask import mask\n",
    "from rasterio import features\n",
    "import math, time\n",
    "import os\n",
    "\n",
    "from rasterio import Affine\n",
    "from rasterio.plot import show\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pickle\n",
    "import json \n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "\n",
    "# Imagehelpers\n",
    "CLOUD_FILTER = 50\n",
    "CLD_PRB_THRESH = 70 # Cloud probability (%); pixel values greater than are considered cloud\n",
    "NIR_DRK_THRESH = 0.2 # Near-infrared reflectance; values less than are considered potential cloud shadow\n",
    "CLD_PRJ_DIST = 5 # Maximum distance (km) to search for cloud shadows from cloud edges\n",
    "BUFFER = 50 # Distance (m) to dilate the edge of cloud-identified objects\n",
    "\n",
    "def remove_ag(img):\n",
    "    afg_adm = ee.FeatureCollection(\"FAO/GAUL/2015/level0\").filter(ee.Filter.eq('ADM0_NAME', 'Afghanistan'))\n",
    "    land2019 = ee.Image(\"COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019\")\n",
    "    afg_ag = land2019.select('discrete_classification')\\\n",
    "    .updateMask(land2019.select(['discrete_classification']).eq(40)).clip(afg_adm)\n",
    "    \n",
    "    return img.updateMask(afg_ag.select('discrete_classification').mask())\n",
    "\n",
    "def mask_edges(img):\n",
    "    \"\"\"\n",
    "    Mask the edges\n",
    "    The masks for the 10m bands sometimes do not exclude bad data at\n",
    "    scene edges, so we apply masks from the 20m and 60m bands as well.\n",
    "    Example asset that needs this operation:\n",
    "    COPERNICUS/S2_CLOUD_PROBABILITY/20190301T000239_20190301T000238_T55GDP\n",
    "    \"\"\"\n",
    "    return img.updateMask(\n",
    "      img.select('B8A').mask().updateMask(img.select('B9').mask()))\n",
    "\n",
    "def remove_cloud_shadow(img):\n",
    "    \n",
    "    \"\"\"\n",
    "    Add cld_prb and is_cloud bands to image\n",
    "    \"\"\"\n",
    "    temp = None\n",
    "    \n",
    "    # Get s2cloudless image, subset the probability band.    \n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.    \n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    temp = img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Add dark_pixels, cld_proj, shadows\n",
    "    \"\"\"\n",
    "    not_water=temp.select('SCL').neq(6)\n",
    "    \n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = temp.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "    \n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(temp.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "    \n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (temp.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': temp.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "    \n",
    "    temp = temp.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Add is_cld_shdw\n",
    "    \"\"\"\n",
    "    \n",
    "     # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = temp.select('clouds').add(temp.select('shadows')).gt(0)\n",
    "    \n",
    "    \n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Update image to only include those pixels with no cloud or shadow\n",
    "    \"\"\"\n",
    "    temp = temp.addBands(is_cld_shdw)\n",
    "    \n",
    "    not_cld_shdw = temp.select('cloudmask').Not()\n",
    "    \n",
    "    return temp.select('B.*').updateMask(not_cld_shdw)\n",
    "    \n",
    "    \n",
    "def add_ndvi(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('ndvi')\n",
    "    return image.addBands([ndvi])\n",
    "\n",
    "def mask_ndvi(image, ltOrGt, value):\n",
    "    ndvi = image.select('ndvi')\n",
    "    if(ltOrGt == 'gt'):\n",
    "        return image.updateMask(ndvi.gt(value))\n",
    "    else: \n",
    "        return image.updateMask(ndvi.lt(value))\n",
    "    \n",
    "def getSentinelCollection(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)) \n",
    "        .map(mask_edges)) # add this \n",
    "    \n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))\n",
    "\n",
    "\n",
    "class RasterGenerationHelper:\n",
    "\n",
    "    def __init__(self, parent_path, child_path,  raster_output_dir, n_cores, clean=False, post_period_days=[30,45]):\n",
    "        self.parent = gpd.read_file(parent_path)\n",
    "        self.child = gpd.read_file(child_path)\n",
    "        self.raster_output_dir = raster_output_dir + \"/\"\n",
    "        self.post_period_days = post_period_days\n",
    "\n",
    "        self.n_cores = n_cores\n",
    "        self._make_dir()\n",
    "        if not clean:\n",
    "            self._ready()\n",
    "            \n",
    "    def get_rasters(self):            \n",
    "        if not os.path.isdir(self.raster_output_dir):\n",
    "            os.makedirs(self.raster_output_dir)\n",
    "            print(\"Created new directory..\".format(self.raster_output_dir))\n",
    "        cpus = self.n_cores\n",
    "#         cpus = 6\n",
    "        parent_chunks = np.array_split(self.parent, cpus)\n",
    "        pool = mp.Pool(processes=cpus)\n",
    "        chunk_processes = [pool.apply_async(self._get_rasters_for_chunk, args=(chunk, self.parent)) for chunk in parent_chunks]\n",
    "        chunk_results = [chunk.get() for chunk in chunk_processes]\n",
    "    \n",
    "    def _get_rasters_for_chunk(self, gdf_chunk, gdf_complete):\n",
    "        \n",
    "        for i, tile in gdf_chunk.iterrows():\n",
    "            temp = self.child.sjoin(self.parent[self.parent['pgrid_id']==tile['pgrid_id']], how=\"inner\", predicate=\"intersects\")\n",
    "#             temp = self.child.sjoin(self.parent)\n",
    "            \n",
    "            \n",
    "            datewise_counts = temp.groupby(['BSD']).count()['grid_id'].sort_values(ascending=False)\n",
    "            datewise_counts = datewise_counts.reset_index()\n",
    "            datewise_counts = datewise_counts.reset_index().rename(columns={'index': 'Date Combo Code', 'grid_id': 'count'})\n",
    "#             print(\"## TILE PGRID_ID present?\", tile['pgrid_id'] in set(list(np.unique(self.parent['pgrid_id']))))\n",
    "#             print(\"## PARENT PGRID_IDs\", self.parent['pgrid_id'])\n",
    "            images = []\n",
    "            for index, row in datewise_counts.iterrows():\n",
    "                \n",
    "                bsd = row['BSD']\n",
    "                \n",
    "#                 bed = row['BED']\n",
    "#                 print(\"###DATE\", date.fromisoformat(bsd).isoformat())\n",
    "                preStart = (date.fromisoformat(bsd) + timedelta(days = -7)).isoformat()\n",
    "                preEnd = (date.fromisoformat(bsd) + timedelta(days = +7)).isoformat()\n",
    "                postStart = (date.fromisoformat(bsd) + timedelta(days = +self.post_period_days[0])).isoformat()\n",
    "                postEnd = (date.fromisoformat(bsd) + timedelta(days = +self.post_period_days[1])).isoformat()\n",
    "                \n",
    "                \n",
    "                tileIDS = temp[(temp['BSD'] == bsd)]['grid_id'].to_list()\n",
    "                \n",
    "                aoiInput = eeconvert.gdfToFc(self.child[self.child['grid_id'].isin(tileIDS)])\n",
    "                \n",
    "                # Get pre\n",
    "                preImage = getSentinelCollection(aoiInput, preStart, preEnd)\n",
    "                pre_w_ndvi = (preImage.map(remove_cloud_shadow).map(add_ndvi).reduce(ee.Reducer.median()))\n",
    "\n",
    "                # Get Post\n",
    "                postImage = getSentinelCollection(aoiInput, postStart, postEnd)\n",
    "                post_w_ndvi = (postImage.map(remove_cloud_shadow).map(add_ndvi).reduce(ee.Reducer.median()))\n",
    "#                 postImage = getSentinelCollection(aoiInput, postStart, postEnd).first()\n",
    "#                 postImage = ee.ImageCollection.fromImages([postImage]);\n",
    "\n",
    "                combined = pre_w_ndvi.addBands([post_w_ndvi])   \n",
    "    \n",
    "                if len(combined.bandNames().getInfo()) > 13:\n",
    "                \n",
    "                    tmpNDVI = combined.select(['ndvi_median']).multiply(5000).rename('ndvi')\n",
    "                    tmpNDVI_1 = combined.select(['ndvi_median_1']).multiply(5000).rename('ndvi1')\n",
    "                    combined = combined.select('B.*', 'ndvi.*')\n",
    "\n",
    "                    # Clip to AOI\n",
    "                    combined_clip = combined.clip(aoiInput)\n",
    "                    images.append(combined_clip)\n",
    "            \n",
    "            bounds = self.parent[self.parent['pgrid_id'] == tile['pgrid_id']].bounds\n",
    "            minx, miny, maxx, maxy = np.max(bounds['minx']), np.max(bounds['miny']),np.max(bounds['maxx']),np.max(bounds['maxy'])\n",
    "            aoi = ee.Geometry.Rectangle([minx, miny, maxx, maxy])\n",
    "\n",
    "            mosaicked = ee.ImageCollection([*images]).mosaic()\n",
    "#             print(\"BandsBrah\", mosaicked.bandNames().getInfo())\n",
    "            geemap.ee_export_image(\n",
    "                mosaicked, \n",
    "                filename=self.raster_output_dir + str(tile['pgrid_id'])+\".tif\", \n",
    "                scale=10, \n",
    "                region=aoi, \n",
    "                file_per_band=False\n",
    "            )\n",
    "            \n",
    "            \n",
    "    def _filename_to_ids(self, filename):\n",
    "        return int(filename.split(\".tif\")[0])\n",
    "    \n",
    "    def _ready(self):\n",
    "        print(list(filter(lambda x: x.endswith(\".tif\"), os.listdir(self.raster_output_dir))))\n",
    "        \n",
    "        files = list(map(self._filename_to_ids, list(filter(lambda x: x.endswith(\".tif\"), os.listdir(self.raster_output_dir)))))\n",
    "        old_size = self.parent.shape[0]\n",
    "        self.parent = self.parent[~self.parent['pgrid_id'].isin(files)]\n",
    "        new_size = self.parent.shape[0]\n",
    "        \n",
    "        print(\"Ignoring {} tiles as rasters for them are already generated; will only generate rasters for remaining {} tiles\".format(old_size - new_size, new_size))\n",
    "        \n",
    "        \n",
    "    def _make_dir(self):\n",
    "        Path(self.raster_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MergeRasterSingleAoi:\n",
    "    \n",
    "    def __init__(self, data_dir, shp_path, tiles_path):\n",
    "        self.shp_path = shp_path\n",
    "        self.tiles_path = tiles_path\n",
    "        self.data_dir = data_dir\n",
    "        self.raster_out_dir = f\"{self.data_dir}/interim\"\n",
    "        \n",
    "    def merge(self, filename=\"merged\"):\n",
    "        aoi = gpd.read_file(self.shp_path)\n",
    "        parent = gpd.read_file(f\"{self.data_dir}/interim/parent.gpkg\")\n",
    "        \n",
    "        aoi = gpd.read_file(self.shp_path)\n",
    "        df = gpd.sjoin(parent, aoi)\n",
    "        rasters = list(self.tiles_path+'/' + (df['pgrid_id']).astype('str') + \".tif\")\n",
    "        \n",
    "        temp_dir = f\"{self.raster_out_dir}/temp\"\n",
    "        \n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for file in rasters:\n",
    "            if os.path.exists(file):\n",
    "                shutil.copy(file, temp_dir+\"/\")\n",
    "\n",
    "        \n",
    "        if os.path.exists(f\"{self.raster_out_dir}/{filename}.vrt\"):\n",
    "            os.remove(f\"{self.raster_out_dir}/{filename}.vrt\")\n",
    "            \n",
    "        if os.path.exists(f\"{self.raster_out_dir}/{filename}.tif\"):\n",
    "            os.remove(f\"{self.raster_out_dir}/{filename}.tif\")\n",
    "            \n",
    "        os.system(f'gdalbuildvrt {self.raster_out_dir}/{filename}.vrt {temp_dir}/*.tif -srcnodata \"0\"')\n",
    "        os.system(f'gdal_merge.py -o {self.raster_out_dir}/{filename}.tif {self.raster_out_dir}/{filename}.vrt')\n",
    "        \n",
    "        raster = rxr.open_rasterio(f'{self.raster_out_dir}/{filename}.tif').squeeze()\n",
    "        raster = raster.rio.clip(aoi.geometry.apply(mapping), aoi.crs)\n",
    "        raster.rio.to_raster(f'{self.raster_out_dir}/{filename}.tif')\n",
    "        \n",
    "        shutil.rmtree(temp_dir)\n",
    "        \n",
    "class MergeRaster:\n",
    "    \"\"\"\n",
    "    Makes analysis ready rasters for a \n",
    "    given shapefile. Assumes vectors and \n",
    "    rasters are already generated for \n",
    "    all of Afghanistan and they are stored \n",
    "    in self.out_dir.\n",
    "    \n",
    "    Asssumes the following files are present:\n",
    "        * district shapefiles in self.aoi_path\n",
    "        * vectors/parent.gpkg\n",
    "        * rasters/bestdates_tiles/*.tif (2500mx2500m GeoTIFFs)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, aoi_path, out_dir, n_cores):\n",
    "        self.aoi_path = aoi_path\n",
    "        self.out_dir = out_dir\n",
    "        self.start_time = time.time()\n",
    "        self.n_cores = n_cores\n",
    "\n",
    "    \n",
    "            \n",
    "    def _handle_chunk(self, aoi_chunk, aoi_complete):\n",
    "        parent = gpd.read_file(self.out_dir + \"/vectors/parent.gpkg\")\n",
    "        RASTER_OUTPUT_DIR = self.out_dir + \"/rasters/bestdates_tiles\"\n",
    "\n",
    "        for file in aoi_chunk:\n",
    "            poly = gpd.read_file(file)\n",
    "            df = gpd.sjoin(parent, poly)\n",
    "            rasters = list(RASTER_OUTPUT_DIR+'/' + (df['pgrid_id']).astype('str') + \".tif\")\n",
    "            dist_id = file.split(\"/districts/\")[1].split(\".gpkg\")[0]\n",
    "            temp_dir = self.out_path + \"/temp_\"+  dist_id\n",
    "            Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for file in rasters:\n",
    "                shutil.copy(file, temp_dir+\"/\")\n",
    "\n",
    "            \n",
    "            print(f\"--- Making VRT for District {dist_id}: {time.time() - self.start_time} seconds ---\")\n",
    "            os.system(f'gdalbuildvrt {temp_dir}/temp_{dist_id}.vrt {temp_dir}/*.tif -srcnodata \"0\"')\n",
    "            print(f\"--- Merging for District {dist_id}: {time.time() - self.start_time} seconds ---\")\n",
    "            os.system(f'gdal_merge.py -o {self.out_path}/merged_{dist_id}.tif {temp_dir}/temp_{dist_id}.vrt')\n",
    "            \n",
    "            print(f\"--- Clipping for District {dist_id}: {time.time() - self.start_time} seconds ---\")\n",
    "            raster = rxr.open_rasterio(f'{self.out_path}/merged_{dist_id}.tif').squeeze()\n",
    "            raster = raster.rio.clip(poly.geometry.apply(mapping), poly.crs)\n",
    "            raster.rio.to_raster(f'{self.out_path}/merged_{dist_id}.tif')\n",
    "\n",
    "    \n",
    "    def merge_rasters(self, out_path): \n",
    "        self.out_path = out_path\n",
    "        aois = list(map(lambda x: self.aoi_path + \"/\" + x, os.listdir(self.aoi_path)))\n",
    "        print(aois)\n",
    "        cpus = self.n_cores\n",
    "        aoi_chunks = np.array_split(aois, cpus)\n",
    "        pool = mp.Pool(processes=cpus)\n",
    "        chunk_processes = [pool.apply_async(self._handle_chunk, args=(chunk, aois)) for chunk in aoi_chunks]\n",
    "        chunk_results = [chunk.get() for chunk in chunk_processes]\n",
    "\n",
    "\n",
    "\n",
    "class Masker:\n",
    "    def __init__(self, data_dir, input_shp, input_raster, mask_raster):\n",
    "        self.input_raster = rasterio.open(input_raster)\n",
    "        self.input_shp = gpd.read_file(input_shp)\n",
    "        self.mask_raster = rasterio.open(mask_raster)\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def mask(self, filename=\"masked\"):\n",
    "        out_img, out_transform = mask(self.mask_raster, shapes=self.input_shp.geometry, crop=True)\n",
    "        out_img[out_img == 0] = 255\n",
    "        is_valid = (out_img != 255.0).astype(np.uint8)\n",
    "        cropland = []\n",
    "        for coords, value in features.shapes(is_valid, transform=out_transform):\n",
    "            if value != 0:\n",
    "                geom = shape(coords)\n",
    "                cropland.append({\"geometry\": geom})\n",
    "                \n",
    "        cropland = gpd.GeoDataFrame(cropland).set_crs(\"epsg:4326\")\n",
    "        out_img, out_transform = mask(self.input_raster, cropland.geometry, crop=True)\n",
    "        out_img[np.isnan(out_img)] = 0\n",
    "        out_img = out_img[0:24]\n",
    "        with rasterio.open(\n",
    "            f'{self.data_dir}/interim/{filename}.tif',\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=out_img.shape[1],\n",
    "            width=out_img.shape[2],\n",
    "            count=out_img.shape[0],\n",
    "            dtype='float32',\n",
    "            crs=self.input_raster.crs,\n",
    "            transform=out_transform,\n",
    "        ) as dst:\n",
    "            dst.write(out_img[0:24])\n",
    "\n",
    "\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, data_dir, input_raster):\n",
    "        self.input_raster = input_raster\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def _add_ndvi(self, df, b8, b4, label):\n",
    "        df[label] = (df[b8] - df[b4]) / (df[b8] + df[b4]) \n",
    "        return df\n",
    "   \n",
    "    def _add_dates(self, df):\n",
    "        child_gdf = gpd.read_file(f\"{self.data_dir}/interim/child.gpkg\")\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.reset_index().x, df.reset_index().y))\n",
    "        gdf = gdf.set_crs(\"epsg:4326\").sjoin(child_gdf)\n",
    "        gdf = gdf.drop(['grid_id', 'geometry'], axis=1)\n",
    "        gdf['dayofyear'] = pd.to_datetime(gdf['BSD']).dt.dayofyear\n",
    "        gdf = gdf.drop('BSD', axis=1)\n",
    "        gdf = gdf.drop(['index_right'], axis=1)\n",
    "        return gdf\n",
    "\n",
    "    def _generate_df_from_raster(self):\n",
    "        with rasterio.open(self.input_raster) as src:\n",
    "            profile = src.profile\n",
    "            \n",
    "        image = rxr.open_rasterio(self.input_raster)\n",
    "        df = image.to_dataframe(name=\"value\")\n",
    "        df = df.reset_index()\n",
    "        df.columns = ['band', 'y', 'x', 'spatial_ref', 'value']\n",
    "        df = pd.pivot(df, index = ['y', 'x'], columns=['band'], values=['value']).reset_index()\n",
    "        print(df.columns)\n",
    "        cols = [*df.columns.get_level_values(0)[0:2], *df.columns.get_level_values(1)[2:]]\n",
    "        df.columns = df.columns.to_flat_index()\n",
    "        df.columns = cols\n",
    "        \n",
    "        \n",
    "        df = df.set_index(['y', 'x'])\n",
    "        df = self._add_ndvi(df, 8, 4, \"ndvi_pre\")\n",
    "        df = self._add_ndvi(df, 20, 16, \"ndvi_post\")\n",
    "        df = self._add_dates(df)\n",
    "        \n",
    "        return df, profile\n",
    "        \n",
    "    def sample(self, sample_size, sample_filename=\"sample\", full_filename=\"full\", save_full = True):\n",
    "        df, profile = self._generate_df_from_raster()\n",
    "        sample_length = int(len(df) * sample_size)\n",
    "        sample = df.sample(sample_length, random_state=7)\n",
    "        sample.to_pickle(f'{self.data_dir}/interim/{sample_filename}.tgz')\n",
    "        if save_full:\n",
    "            df.to_pickle(f'{self.data_dir}/interim/{full_filename}.tgz')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca37fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "class DataHelper:\n",
    "    def __init__(self, data_dir, raw):\n",
    "        self.data_dir = data_dir\n",
    "        self.raw = pd.read_pickle(raw)\n",
    "        \n",
    "    def pre_only(self):\n",
    "        return self.raw[[*range(1,13),'ndvi_pre', 'dayofyear']]\n",
    "    \n",
    "    def post_only(self):\n",
    "        return self.raw[[*range(12,25),'ndvi_pre','ndvi_post', 'dayofyear']]\n",
    "    \n",
    "    def diff_bands(self):\n",
    "        copy = self.raw.copy()\n",
    "        for col in range(1,13):\n",
    "            copy[col] = copy[col+12] - copy[col]\n",
    "        return copy[[*range(1,13), 'ndvi_pre','dayofyear']]\n",
    "    \n",
    "    def diff_all(self):\n",
    "        copy = self.raw.copy()\n",
    "        for col in range(1,13):\n",
    "            copy[col] = copy[col+12] - copy[col]\n",
    "        copy['diff_ndvi'] = copy['ndvi_post'] - copy['ndvi_pre']\n",
    "        return copy[[*range(1,13), 'ndvi_pre', 'diff_ndvi', 'dayofyear']]\n",
    "    \n",
    "    def ndvi_and_day(self):\n",
    "        return self.raw[['ndvi_pre', 'dayofyear']]\n",
    "    \n",
    "    def save(self, df, label, filename=None):\n",
    "        print(f\"------ Saved {label}; Columns: {df.columns}\")\n",
    "        if filename != None:\n",
    "            df.to_pickle(filename)\n",
    "        else:\n",
    "            df.to_pickle(f'{self.data_dir}/interim/data_{label}.tgz')\n",
    "        \n",
    "class ModelingHelper:\n",
    "    def __init__(self, data_dir, raw, run_name):\n",
    "        self.raw = pd.read_pickle(raw)\n",
    "        self.data_dir = data_dir\n",
    "        self.run_name = run_name\n",
    "        with rasterio.open(f\"{self.data_dir}/interim/temp.tif\") as src:\n",
    "            self.profile = src.profile\n",
    "    \n",
    "    def ready_data(self, data):\n",
    "        dataset = data[data['ndvi_pre'] > 0.3]\n",
    "        dataset = dataset.loc[~(dataset==0).all(axis=1)]\n",
    "        dataset.columns = dataset.columns.astype('str')\n",
    "        return dataset\n",
    "        \n",
    "    def fit(self, data, model_type, n, save=True, drop_ndvi=False):\n",
    "        clean = self.ready_data(data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(clean)\n",
    "        normalised_data = scaler.transform(clean)\n",
    "        \n",
    "        if model_type=='kmeans':\n",
    "            model = KMeans(n_clusters=n)\n",
    "            model.fit(normalised_data)\n",
    "\n",
    "        if model_type=='gmm':\n",
    "            model = GaussianMixture(n, covariance_type='full', random_state=0)\n",
    "            model.fit(normalised_data)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        MODEL_DIR = f'{self.data_dir}/outputs/models/{self.run_name}'\n",
    "        Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(f'{MODEL_DIR}/model.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        with open(f'{MODEL_DIR}/scaler.pickle', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def load_scaler(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "    \n",
    "        return scaler\n",
    "    \n",
    "    def predict(self, data, model, scaler):\n",
    "        dataset = self.ready_data(data)\n",
    "        cluster_assignments =  model.predict(scaler.transform(dataset))\n",
    "        result = dataset.copy()\n",
    "        result['clust'] = cluster_assignments\n",
    "        result = result[['clust']]\n",
    "        result = pd.merge(data.reset_index(), result.reset_index(), how=\"left\").set_index(['y','x'])\n",
    "        return result\n",
    "        \n",
    "    def save_raster(self, results, filename):\n",
    "        crs = self.profile['crs']\n",
    "        transform = self.profile['transform']\n",
    "        \n",
    "        clust_assignments = pd.DataFrame(results['clust'])\n",
    "        clust_assignments = pd.melt(clust_assignments, value_vars=['clust'], value_name='value', ignore_index=False)   \n",
    "        clust_assignments = clust_assignments.drop('variable', axis=1) \n",
    "        clust_assignments[np.isnan(clust_assignments['value'])] = -99\n",
    "#         return clust_assignments\n",
    "        clust_assignments = clust_assignments.reset_index().drop_duplicates(subset=['y', 'x']).set_index(['y', 'x']).to_xarray()\n",
    "        clust_assignments.rio.to_raster(f\"{filename}.tif\")\n",
    "        \n",
    "        with rasterio.open(f\"{filename}.tif\", \"r+\") as src:\n",
    "            src.crs = crs\n",
    "            src.nodata=-99\n",
    "    \n",
    "    def save_ndvi_plot(self, results, model_type, n, filename):\n",
    "        fig, ax = plt.subplots(n, 1, dpi=70, figsize=(9,9))\n",
    "        ax=ax.flatten()\n",
    "            \n",
    "        for i in range(0,n):\n",
    "            data = results[results['clust']==i]['ndvi_pre']\n",
    "            ax[i].hist(data, bins=100)\n",
    "            ax[i].set_title(f\"{model_type} with k={n}; cluster={i}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{filename}.png\")\n",
    "        plt.close('all')\n",
    "        \n",
    "    def get_poppy_pixels(self, dist_id, year):\n",
    "        poppyPixels = pd.read_csv(self.data_dir + \"/inputs/poppy_1994-2020.csv\")\n",
    "        poppyPixels = poppyPixels[poppyPixels['distid'] == dist_id]\n",
    "\n",
    "        return poppyPixels[f'X{year}'].iloc[0] \n",
    "\n",
    "    def save_comparison_results(self, results, dist, year, filename):\n",
    "        results = pd.DataFrame(results.groupby('clust').count()['ndvi_pre']).rename(columns={'ndvi_pre': 'pixels_from_clustering'})\n",
    "        results['clustering_ha'] = results['pixels_from_clustering']/100\n",
    "#         results['unodc_ha'] = self.get_poppy_pixels(dist, year)\n",
    "        results.to_csv(f\"{filename}.csv\")\n",
    "        print(f\"--------- Comparison saved to {filename}.csv\")\n",
    "        \n",
    "class PredictionHelper:\n",
    "    def _init_():\n",
    "        print(\"Init\")\n",
    "    def prep_raster():\n",
    "        print(\"Prep raster\")\n",
    "    def load_models():\n",
    "        print(\"Load raster\")\n",
    "    def predict():\n",
    "        print(\"Predict\")\n",
    "    def save():\n",
    "        print(\"Save\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c93ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import gc\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"data_dir\")\n",
    "# parser.add_argument(\"shp_path\")\n",
    "# parser.add_argument(\"model_path\")\n",
    "# parser.add_argument(\"data_file_suffix\")\n",
    "# # parser.add_argument(\"--tiles_path\")\n",
    "# parser.add_argument(\"--model_type\")\n",
    "# parser.add_argument(\"--num\")\n",
    "# parser.add_argument(\"--year\")\n",
    "# parser.add_argument(\"--name\")\n",
    "# parser.add_argument(\"--dist\")\n",
    "# parser.add_argument(\"--skip_raster_generation\", action=argparse.BooleanOptionalAction)\n",
    "# args = parser.parse_args()       \n",
    "\n",
    "\n",
    "\n",
    "# from utils.rasters import MergeRasterSingleAoi, Masker, Sampler\n",
    "# from utils.data import ModelingHelper, DataHelper\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# DATA_DIR = args.data_dir\n",
    "# SHP_PATH = args.shp_path\n",
    "# MODEL_PATH = args.model_path\n",
    "# DATA_FILE_SUFFIX = args.data_file_suffix\n",
    "# TILES_PATH = f\"{DATA_DIR}/interim/tiles\"\n",
    "# YEAR = '2019'\n",
    "# NAME = MODEL_PATH.split(\"/\")[-1]\n",
    "# SKIP_RASTER_GENERATION = False\n",
    "# N = 3\n",
    "# MODEL_TYPE = 'kmeans'\n",
    "# DIST = '2308'\n",
    "\n",
    "# if args.year:\n",
    "#     YEAR = args.year\n",
    "# if args.name:\n",
    "#     NAME = args.name\n",
    "# if args.skip_raster_generation:\n",
    "#     SKIP_RASTER_GENERATION = True\n",
    "# if args.model_type:\n",
    "#     MODEL_TYPE=args.model_type\n",
    "# if args.num:\n",
    "#     N=int(args.num)\n",
    "# if args.dist:\n",
    "#     DIST = args.dist\n",
    "# # if args.tiles_path:\n",
    "# #     TILES_PATH = arg.tiles_path\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07300938",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "SHP_PATH = \"/data/tmp/arogya/data/inputs/aoi.gpkg\"\n",
    "MODEL_PATH = \"/data/tmp/arogya/data/outputs/models/kmeans_3_diff_bands/\"\n",
    "DATA_FILE_SUFFIX = \"diff_bands\"\n",
    "TILES_PATH = f\"{DATA_DIR}/interim/tiles\"\n",
    "YEAR = '2019'\n",
    "NAME = MODEL_PATH.split(\"/\")[-1]\n",
    "SKIP_RASTER_GENERATION = False\n",
    "N = 3\n",
    "MODEL_TYPE = 'kmeans'\n",
    "DIST = '2308'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e62beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Merger\n",
      "In Masker\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not SKIP_RASTER_GENERATION:\n",
    "    \n",
    "    \n",
    "    if os.path.exists(f\"{DATA_DIR}/interim/temp\"):\n",
    "        shutil.rmtree(f\"{DATA_DIR}/interim/temp\")\n",
    "        \n",
    "    if os.path.exists(f\"{DATA_DIR}/interim/temp.vrt\"):\n",
    "        os.remove(f\"{DATA_DIR}/interim/temp.vrt\")\n",
    "    \n",
    "    if os.path.exists(f\"{DATA_DIR}/interim/temp.tif\"):\n",
    "        os.remove(f\"{DATA_DIR}/interim/temp.tif\")\n",
    "    \n",
    "    if os.path.exists(f\"{DATA_DIR}/interim/merged.tif\"):\n",
    "        os.remove(f\"{DATA_DIR}/interim/merged.tif\")\n",
    "        \n",
    "    if os.path.exists(f\"{DATA_DIR}/interim/masked.tif\"):\n",
    "        os.remove(f\"{DATA_DIR}/interim/masked.tif\")\n",
    "    \n",
    "    \n",
    "    print(\"In Merger\")\n",
    "    \n",
    "    # Create mereged raster for AOI\n",
    "    mrs = MergeRasterSingleAoi(DATA_DIR, SHP_PATH, TILES_PATH)\n",
    "    mrs.merge(\"temp\")\n",
    "\n",
    "    # Create Masked Raster for AOI\n",
    "    _CROP_MASK_PATH = f'{DATA_DIR}/inputs/{YEAR}_E060N40_PROBAV_LC100_global_v3.0.1_2019.tif'\n",
    "    _INPUT_RASTER_PATH = f'{DATA_DIR}/interim/temp.tif'\n",
    "    print(\"In Masker\")\n",
    "\n",
    "    masker = Masker(DATA_DIR, SHP_PATH, _INPUT_RASTER_PATH, _CROP_MASK_PATH)\n",
    "    masker.mask(\"temp\")\n",
    "\n",
    "# print(\"In Sampler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2348fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sampler\n",
      "MultiIndex([(    'y', ''),\n",
      "            (    'x', ''),\n",
      "            ('value',  1),\n",
      "            ('value',  2),\n",
      "            ('value',  3),\n",
      "            ('value',  4),\n",
      "            ('value',  5),\n",
      "            ('value',  6),\n",
      "            ('value',  7),\n",
      "            ('value',  8),\n",
      "            ('value',  9),\n",
      "            ('value', 10),\n",
      "            ('value', 11),\n",
      "            ('value', 12),\n",
      "            ('value', 13),\n",
      "            ('value', 14),\n",
      "            ('value', 15),\n",
      "            ('value', 16),\n",
      "            ('value', 17),\n",
      "            ('value', 18),\n",
      "            ('value', 19),\n",
      "            ('value', 20),\n",
      "            ('value', 21),\n",
      "            ('value', 22),\n",
      "            ('value', 23),\n",
      "            ('value', 24),\n",
      "            ('value', 25),\n",
      "            ('value', 26)],\n",
      "           names=[None, 'band'])\n",
      "------ Saved diff_bands; Columns: Index([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 'ndvi_pre', 'dayofyear'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"In Sampler\")\n",
    "sampler = Sampler(DATA_DIR, f'{DATA_DIR}/interim/temp.tif')\n",
    "sampler.sample(1, sample_filename = \"temp\", save_full=False)\n",
    "\n",
    "dh = DataHelper(DATA_DIR, f'{DATA_DIR}/interim/temp.tgz')\n",
    "\n",
    "if DATA_FILE_SUFFIX==\"diff_bands\":\n",
    "    dh.save(dh.diff_bands(), \"diff_bands\", filename=f'{DATA_DIR}/interim/temp.tgz')\n",
    "if DATA_FILE_SUFFIX==\"pre_only\":\n",
    "    dh.save(dh.pre_only(), \"pre_only\", filename=f'{DATA_DIR}/interim/temp.tgz')\n",
    "if DATA_FILE_SUFFIX==\"post_only\":\n",
    "    dh.save(dh.post_only(), \"post_only\", filename=f'{DATA_DIR}/interim/temp.tgz')\n",
    "if DATA_FILE_SUFFIX==\"ndvi_and_day\":\n",
    "    dh.save(dh.ndvi_and_day(), \"ndvi_and_day\", filename=f'{DATA_DIR}/interim/temp.tgz')\n",
    "if DATA_FILE_SUFFIX==\"diff_all\":\n",
    "    dh.save(dh.ndvi_and_day(), \"diff_all\", filename=f'{DATA_DIR}/interim/temp.tgz')\n",
    "\n",
    "    \n",
    "    \n",
    "_DATA_FILE_PATH = f'{DATA_DIR}/interim/temp.tgz'\n",
    "\n",
    "\n",
    "# Path(f\"{DATA_DIR}/outputs/predictions/{NAME}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# mh.save_raster(results, f\"{DATA_DIR}/outputs/predictions/{NAME}/{NAME}\")\n",
    "# mh.save_comparison_results(results, DIST, YEAR, f\"{DATA_DIR}/outputs/predictions/{NAME}/{NAME}\")\n",
    "# mh.save_ndvi_plot(results, MODEL_TYPE, N, f\"{DATA_DIR}/outputs/predictions/{NAME}/{NAME}\")\n",
    "\n",
    "# mrs=None\n",
    "# masker=None\n",
    "# sampler=None\n",
    "# dh=None\n",
    "# mh=None\n",
    "# model=None\n",
    "# scaler=None\n",
    "# results=None\n",
    "# gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70aa6be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh = ModelingHelper(DATA_DIR, _DATA_FILE_PATH, \"dummy\")\n",
    "model = mh.load_model(f\"{MODEL_PATH}/model.pickle\")\n",
    "scaler = mh.load_scaler(f\"{MODEL_PATH}/scaler.pickle\")\n",
    "\n",
    "\n",
    "results = mh.predict(mh.raw, model, scaler)\n",
    "results.reset_index().drop_duplicates(subset=['y', 'x']).set_index(['y', 'x'])\n",
    "\n",
    "Path(f\"{DATA_DIR}/outputs/predictions/{NAME}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clust_assignments = mh.save_raster(results, f\"{DATA_DIR}/outputs/predictions/{NAME}/{NAME}\")\n",
    "type(clust_assignments)\n",
    "# mh.save_comparison_results(results, DIST, YEAR, f\"{DATA_DIR}/outputs/predictions/{NAME}/{NAME}\")\n",
    "# mh.save_ndvi_plot(results, MODEL_TYPE, N, f\"{DATA_DIR}/outputs/predictions/{NAME}/{NAME}\")\n",
    "\n",
    "# mrs=None\n",
    "# masker=None\n",
    "# sampler=None\n",
    "# dh=None\n",
    "# mh=None\n",
    "# model=None\n",
    "# scaler=None\n",
    "# results=None\n",
    "# gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a0f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_assignments.reset_index().drop_duplicates(subset=['y', 'x']).set_index(['y', 'x']).to_xarray().rio.to_raster(\"test.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4576bbb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert a DataFrame with a non-unique MultiIndex into xarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclust_assignments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_xarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/poppy-linux/lib/python3.9/site-packages/pandas/core/generic.py:3178\u001b[0m, in \u001b[0;36mNDFrame.to_xarray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xarray\u001b[38;5;241m.\u001b[39mDataArray\u001b[38;5;241m.\u001b[39mfrom_series(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/poppy-linux/lib/python3.9/site-packages/xarray/core/dataset.py:6011\u001b[0m, in \u001b[0;36mDataset.from_dataframe\u001b[0;34m(cls, dataframe, sparse)\u001b[0m\n\u001b[1;32m   6008\u001b[0m idx \u001b[38;5;241m=\u001b[39m remove_unused_levels_categories(dataframe\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   6010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, pd\u001b[38;5;241m.\u001b[39mMultiIndex) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 6011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   6012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert a DataFrame with a non-unique MultiIndex into xarray\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6013\u001b[0m     )\n\u001b[1;32m   6015\u001b[0m \u001b[38;5;66;03m# Cast to a NumPy array first, in case the Series is a pandas Extension\u001b[39;00m\n\u001b[1;32m   6016\u001b[0m \u001b[38;5;66;03m# array (which doesn't have a valid NumPy dtype)\u001b[39;00m\n\u001b[1;32m   6017\u001b[0m \u001b[38;5;66;03m# TODO: allow users to control how this casting happens, e.g., by\u001b[39;00m\n\u001b[1;32m   6018\u001b[0m \u001b[38;5;66;03m# forwarding arguments to pandas.Series.to_numpy?\u001b[39;00m\n\u001b[1;32m   6019\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [(k, np\u001b[38;5;241m.\u001b[39masarray(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dataframe\u001b[38;5;241m.\u001b[39mitems()]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert a DataFrame with a non-unique MultiIndex into xarray"
     ]
    }
   ],
   "source": [
    "clust_assignments.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f5ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
